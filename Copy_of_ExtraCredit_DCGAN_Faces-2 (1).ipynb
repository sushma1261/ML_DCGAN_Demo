{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qdxsjoXsfIw"
   },
   "source": [
    "Extra Credit Assignment :  Complete this Notebook to implement a DCGAN similar to what is described in the paper \"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\" by Radford et al. (2016).https://arxiv.org/abs/1511.06434v2  \n",
    "\n",
    "General guidelines\n",
    "1. MAKE A COPY OF THIS NOTEBOOK TEMPLATE TO YOUR OWN DRIVE BEFORE REVISING. You can download your version of the notebook and submit it to CANVAS as a .ipynb notebook file\n",
    "2. You will be using a celebrity faces dataset as input from kaggle and import code for this dataset is provided below\n",
    "3. Do not exceed 8 epochs as this will increase the runtime beyond what is reasonable for extra credit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5wBmS6lsfIx"
   },
   "source": [
    "### Discriminator module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorboardX in /opt/homebrew/lib/python3.9/site-packages (2.6.2.2)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.9/site-packages (from tensorboardX) (1.24.3)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.9/site-packages (from tensorboardX) (24.1)\n",
      "Requirement already satisfied: protobuf>=3.20 in /opt/homebrew/lib/python3.9/site-packages (from tensorboardX) (5.29.0)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvkfydqwsfIx"
   },
   "source": [
    "###  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "c6xFrqMksfIx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from tensorboardX import SummaryWriter\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "8S-W98IFsfIx"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, feature_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # Input: img_channels x 64 x 64\n",
    "            nn.Conv2d(channels_img, feature_d, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            #, inplace=True\n",
    "            # Output: feature_d x 32 x 32\n",
    "            nn.Conv2d(feature_d, feature_d * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(feature_d * 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # Output: (feature_d*2) x 16 x 16\n",
    "            nn.Conv2d(feature_d * 2, feature_d * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(feature_d * 4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # Output: (feature_d*4) x 8 x 8\n",
    "            nn.Conv2d(feature_d * 4, feature_d * 8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(feature_d * 8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # Output: (feature_d*8) x 4 x 4\n",
    "            nn.Conv2d(feature_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "            \n",
    "            # Output: 1 x 1 x 1\n",
    "            nn.Sigmoid(),  \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDVZ1Jc9sfIx"
   },
   "source": [
    "### Generator Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "n-kJbHX_sfIx"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            # Input: N x channels_noise x 1 x 1\n",
    "            nn.ConvTranspose2d(channels_noise, features_g * 16, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(features_g * 16),\n",
    "            nn.ReLU(),\n",
    "            # Output: N x (features_g * 16) x 4 x 4\n",
    "            nn.ConvTranspose2d(features_g * 16, features_g * 8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(features_g * 8),\n",
    "            nn.ReLU(),\n",
    "            # Output: N x (features_g * 8) x 8 x 8\n",
    "            nn.ConvTranspose2d(features_g * 8, features_g * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(features_g * 4),\n",
    "            nn.ReLU(),\n",
    "            # Output: N x (features_g * 4) x 16 x 16\n",
    "            nn.ConvTranspose2d(features_g * 4, features_g * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(features_g * 2),\n",
    "            nn.ReLU(),\n",
    "            # Output: N x (features_g * 2) x 32 x 32\n",
    "            nn.ConvTranspose2d(features_g * 2, channels_img, kernel_size=4, stride=2, padding=1),\n",
    "            # Output: N x channels_img x 64 x 64\n",
    "            nn.Tanh(),   #[-1,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def initialize_weights(model):\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "                init.normal_(m.weight.data, 0.0, 0.02)\n",
    "def test():\n",
    "    N, channels_img, H, W = 8, 3, 64, 64\n",
    "    channels_noise = 100\n",
    "    x = torch.randn((N, channels_img, H, W))\n",
    "    discriminator = Discriminator(channels_img, 8)\n",
    "    initialize_weights(discriminator)\n",
    "    assert discriminator(x).shape == (N, 1, 1, 1)\n",
    "    generator = Generator(channels_noise, channels_img, 8)\n",
    "    initialize_weights(generator)\n",
    "    noise = torch.randn((N, channels_noise, 1, 1))\n",
    "    assert generator(noise).shape == (N, channels_img, H, W)\n",
    "    print(\"Success\")\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIjN0jlSsfIy"
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "BqWeXga7sfIy",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (disc): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remember to specify CPU instead of GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 2e-4\n",
    "batch_size = 128\n",
    "IMAGE_SIZE = 64\n",
    "channels_img = 3\n",
    "channels_noise = 100\n",
    "NUM_EPOCHS = 1 #Do not exceed 10 for your final submission\n",
    "features_d = 64\n",
    "features_g = 64\n",
    "\n",
    "generator = Generator(channels_noise, channels_img, features_g).to(device)\n",
    "discriminator = Discriminator(channels_img, features_d).to(device)\n",
    "\n",
    "initialize_weights(generator)\n",
    "initialize_weights(discriminator)\n",
    "\n",
    "gen_optimizer = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "disc_optimizer = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "\n",
    "# real_label = 1\n",
    "# fake_label = 0\n",
    "\n",
    "fix_noise = torch.randn(32, channels_noise, 1, 1).to(device)\n",
    "\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "step = 0\n",
    "\n",
    "generator.train()\n",
    "discriminator.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7aBGoUlsfIy"
   },
   "source": [
    "### Data-Loading and Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IlCVWpQxsfIy",
    "outputId": "bf8dc3b7-b722-413e-9986-40b6ee9a4a64",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Specify the path to the destination directory\n",
    "# destination_path = \"/content/drive/MyDrive/CSS581-ML-Aut24/InputDataSets/img_align_celeba/img_align_celeba\"\n",
    "destination_path = \"img_align_celeba\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),   # Resize images to (64, 64)\n",
    "    transforms.ToTensor(),            # Convert image to tensor with range [0, 1]\n",
    "    transforms.Normalize(\n",
    "        [0.5 for _ in range(channels_img)], [0.5 for _ in range(channels_img)]),\n",
    "])\n",
    "\n",
    "# Update the root path to the extracted directory\n",
    "dataset = datasets.ImageFolder(root=destination_path, transform=transform)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfO66vFcsfIy"
   },
   "source": [
    "### Define Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "I0gZcbqesfIy"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsDF4EsFsfIy"
   },
   "source": [
    "### Define Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Prl4Bsh-sfIy"
   },
   "source": [
    "#### Seed the randomizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7yvKuLS25J3",
    "outputId": "e1f69649-3d7b-49d3-9bf4-8cc2af7ee2dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [0/1] Batch 0/1583             Loss D: 1.3879, Loss G: 0.8136\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "        for batch_idx, (data, _) in enumerate(dataloader):\n",
    "          data = data.to(device)\n",
    "          noise = torch.randn(batch_size, channels_noise, 1, 1).to(device)\n",
    "          fake = generator(noise)\n",
    "          \n",
    "          ###Train discriminator: max log(D(x)) + log(1-D(G(z)))\n",
    "          disc_real = discriminator(data).reshape(-1)\n",
    "          lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "\n",
    "          disc_fake = discriminator(fake).reshape(-1)\n",
    "          lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake)) \n",
    "\n",
    "          lossD = (lossD_real + lossD_fake) \n",
    "          discriminator.zero_grad()\n",
    "          lossD.backward(retain_graph=True)\n",
    "          disc_optimizer.step()\n",
    "\n",
    "          ###Train generator: max log(D(G(z)))\n",
    "          output = discriminator(fake).reshape(-1)\n",
    "          lossG = criterion(output, torch.ones_like(output))\n",
    "          generator.zero_grad()\n",
    "          lossG.backward()\n",
    "          gen_optimizer.step()\n",
    "\n",
    "          if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "            Loss D: {lossD:.4f}, Loss G: {lossG:.4f}\")\n",
    "            with torch.no_grad():\n",
    "                fake = generator(fix_noise)\n",
    "                img_grid_real = torchvision.utils.make_grid(data[:32], normalize=True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step = step)\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcOjVvD-sfIz"
   },
   "source": [
    "### Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "4tTazvcasfIz"
   },
   "outputs": [],
   "source": [
    "def display_image(epoch_no):\n",
    "     filename = f'generated_images_epoch_{epoch_no}.png'\n",
    "     try:\n",
    "        # Load the image\n",
    "        img = Image.open(filename)\n",
    "\n",
    "        # Display the image using matplotlib\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(transforms.ToPILImage()(transforms.ToTensor()(img)))\n",
    "        plt.axis('off')  # Turn off axis for better visualization\n",
    "        plt.title(f'Generated Images - Epoch {epoch_no}')\n",
    "        plt.show()\n",
    "     except FileNotFoundError:\n",
    "        print(f\"No image found for epoch {epoch_no}. Ensure the file '{filename}' exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "I0kN_DpisfIz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No image found for epoch 5. Ensure the file 'generated_images_epoch_5.png' exists.\n"
     ]
    }
   ],
   "source": [
    "display_image(NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T14:00:13.967297Z",
     "iopub.status.busy": "2024-08-29T14:00:13.966968Z",
     "iopub.status.idle": "2024-08-29T14:00:14.010496Z",
     "shell.execute_reply": "2024-08-29T14:00:14.009053Z",
     "shell.execute_reply.started": "2024-08-29T14:00:13.967271Z"
    },
    "id": "y0vOUnIosfIz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 29561,
     "sourceId": 37705,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
